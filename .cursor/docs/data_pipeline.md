# 데이터 파이프라인 설계서

## 1. 개요

본 문서는 캠샤프트 제조 라인에서 생성되는 CSV 데이터를 수집, 파싱, 정제, 변환하여 데이터베이스에 저장하는 전 과정을 정의하는 데이터 파이프라인의 설계서입니다. 안정적이고 효율적인 데이터 처리를 통해 불량률 예측 모델에 필요한 정제된 데이터를 공급하는 것을 목표로 합니다.

## 2. 데이터 수집 (Data Ingestion)

- **수집 대상:** `data/` 디렉토리 하위에 연/월 구조로 저장된 모든 `Cam_YYYYMMDD.csv` 파일.
- **수집 주체:** `scripts/load_data.py` 파이썬 스크립트.
- **수집 방식:**
  1.  스크립트는 `data` 디렉토리 내의 모든 CSV 파일을 재귀적으로 탐색합니다.
  2.  `oracledb` 라이브러리를 사용하여 데이터베이스에 직접 연결합니다.
  3.  각 CSV 파일의 내용 중 초기 헤더 3줄을 제외한 순수 데이터 행들만 읽어 `HANDY_ZSCORE_RAW_DATA` 테이블에 적재합니다.
  4.  이 과정에서 `id` 값은 `handy_zscore_raw_data_seq` 시퀀스를 통해 애플리케이션 레벨에서 직접 할당됩니다.
  5.  `executemany`를 사용하여 대량 삽입 성능을 최적화합니다.

## 3. 데이터 파싱 및 정제 (Parsing & Cleansing)

- **파싱 주체:** FastAPI 백엔드 애플리케이션 (데이터 분석 API 요청 시)
- **파싱 시점:** `HANDY_ZSCORE_RAW_DATA`에 저장된 원본 데이터를 API를 통해 조회할 때, `HANDY_ZSCORE_COLUMN_MAPPER`를 참조하여 의미있는 데이터로 변환합니다.

### 3.1. 파싱의 복잡성 및 해결 (애플리케이션 레벨)

- **와이드 포맷 데이터:** `HANDY_ZSCORE_RAW_DATA` 테이블은 150개의 `d` 컬럼을 가진 와이드 포맷입니다.
- **동적 매핑:** 애플리케이션은 `HANDY_ZSCORE_COLUMN_MAPPER` 테이블을 조회하여 `d000`이 `barcode`를 의미하고, `d007`이 `cam1_press_force_max`를 의미한다는 것을 동적으로 해석합니다.
- 이를 통해 원본 데이터의 컬럼 순서나 의미가 변경되더라도, 매퍼 테이블의 정보만 수정하면 되므로 애플리케이션 코드 변경을 최소화할 수 있습니다.

### 3.2. 데이터 유효성 검사 및 정제

- **데이터 유효성 검사 (Data Validation):**
  - 핵심 측정값(위상각, 토크, 압입력 등) 컬럼이 숫자형(numeric) 데이터로 변환 가능한지 검증합니다. ('OK', 'NG' 등의 문자열 제외)
  - `d003` (timestamp) 컬럼이 유효한 날짜/시간 형식인지 확인하고, `datetime` 객체로 변환합니다.
- **데이터 정제 (Data Cleansing):**
  - 분석에 불필요한 '판정' 컬럼(`..._Result`) 등은 제거하거나 별도로 처리합니다.
  - 결측치(NaN)는 0으로 채우거나(`fillna(0)`), 데이터 특성에 따라 적절한 값으로 채웁니다.

## 4. 데이터 변환 및 저장 (Data Transformation & Storage)

- **변환 주체:** FastAPI 백엔드 애플리케이션 (데이터 분석 API 요청 시)
- **데이터 변환 (Wide to Long):**
- 파싱된 116개 컬럼의 와이드 포맷(wide format) 데이터를 분석 및 저장이 용이한 롱 포맷(long format)으로 변환합니다.
- **예시:** `CAM1_Angle`, `CAM2_Angle`, ... `CAM9_Angle` 컬럼들을 `product_id`, `cam_number`, `angle` 세 개의 컬럼을 가진 여러 행으로 변환 (Unpivot/Melt).
- 이 변환 과정을 통해 `database_schema.md`에 정의된 정규화된 테이블 구조에 맞게 데이터를 재구성합니다.
- **데이터 저장:**
  - **ORM:** `SQLAlchemy`를 사용하여 데이터베이스와의 상호작용을 처리함으로써 Oracle과 MariaDB 간의 호환성을 확보합니다.
  - **저장 방식:** 대량의 데이터를 효율적으로 삽입하기 위해 `bulk insert` 또는 `chunk-wise` 저장 방식을 사용합니다.
  - 트랜잭션을 적용하여 데이터 삽입 과정에서의 원자성을 보장합니다.

## 5. 오류 처리 및 로깅 (Error Handling & Logging)

- **오류 처리:** 파이프라인의 각 단계(수집, 파싱, 변환, 저장)에서 발생하는 모든 예외는 `try-except` 블록으로 처리합니다.
- **로깅:**
  - 오류 발생 시, 발생 시간, 파일 경로, 오류 메시지, 스택 트레이스를 포함한 상세 정보를 로그 파일에 기록합니다.
  - 정상 처리된 파일에 대해서도 처리 시간, 삽입된 행의 수 등을 정보 로그로 기록하여 파이프라인의 동작 상태를 모니터링할 수 있도록 합니다.
- **실패 처리:** 실패한 파일은 'failed' 폴더로 이동시키고, 재처리를 위한 방안을 마련합니다.
