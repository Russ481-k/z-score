# 데이터 파이프라인 설계서

## 1. 개요

본 문서는 캠샤프트 제조 라인에서 생성되는 CSV 데이터를 수집, 파싱, 정제, 변환하여 데이터베이스에 저장하는 전 과정을 정의하는 데이터 파이프라인의 설계서입니다. 안정적이고 효율적인 데이터 처리를 통해 불량률 예측 모델에 필요한 정제된 데이터를 공급하는 것을 목표로 합니다.

## 2. 데이터 수집 (Data Collection)

- **수집 대상:** `data/{YYYY}/{MM}/Cam_{YYYYMMDD}.csv` 형태로 저장되는 모든 CSV 파일
- **수집 방식:** FastAPI 애플리케이션 내에서 `APScheduler`와 같은 스케줄링 라이브러리를 사용하여 주기적으로 새 파일을 탐지하는 백그라운드 프로세스를 실행합니다.
  - **스케줄:** 매 10분 간격으로 실행하는 것을 기본으로 하며, 운영 상황에 따라 조정 가능합니다.
- **수집 전략:**
  - 마지막으로 처리된 파일의 경로 또는 최종 수정 시간을 데이터베이스나 별도의 로그 파일에 기록합니다.
  - 스케줄러 실행 시, 기록된 시간 이후에 생성되었거나 수정된 파일만 수집 대상으로 선정하여 중복 처리를 방지합니다.

## 3. 데이터 파싱 및 전처리 (Data Parsing & Pre-processing)

### 3.1. 파싱의 복잡성

- **헤더-데이터 불일치:** 원본 CSV는 2번째 줄 헤더(78 필드)와 3번째 줄 이하 데이터(116 필드)의 필드 수가 일치하지 않습니다.
- **복합 컬럼:** 헤더의 'CAM1'과 같은 단일 필드가 실제로는 '최고압입력', '최종압입력', '판정' 등 여러 데이터 컬럼에 대응됩니다.
- 따라서, 표준 `read_csv` 헤더 인식 기능만으로는 정확한 파싱이 불가능합니다.

### 3.2. 파싱 전략: 커스텀 헤더 생성

1.  **헤더 없이 데이터 로드:** `pandas.read_csv`를 `header=None` 옵션으로 호출하여 모든 내용을 데이터로 읽어들입니다. 첫 번째와 두 번째 줄은 별도로 처리하기 위해 건너뜁니다 (`skiprows=[0, 1]`).
2.  **헤더 정보 추출:** 파일의 첫 번째, 두 번째 줄을 별도로 읽어 헤더 생성을 위한 정보로 사용합니다.
3.  **커스텀 헤더 생성 로직:**
    - 1, 2번째 줄의 정보를 조합하여 116개 데이터 필드에 매핑될 상세한 컬럼명 리스트를 프로그래매틱하게 생성합니다.
    - 예시:
      - 'CAM1' + '최고압입력' -> `CAM1_Press_Force_Max`
      - 'CAM1' + '최종압입력' -> `CAM1_Press_Force_Final`
      - 'CAM1' + '판정' -> `CAM1_Press_Result`
      - 'CAM1' + '토크(N.m)' -> `CAM1_Torque`
      - 'CAM1' + '위상각도(도)' -> `CAM1_Angle`
    - 이러한 규칙을 모든 컬럼에 적용하여 116개의 고유하고 의미 있는 컬럼명을 생성합니다.
4.  **헤더 적용:** 생성된 커스텀 헤더 리스트를 데이터프레임의 `columns` 속성에 할당합니다.

### 3.3. 데이터 유효성 검사 및 정제

- **데이터 유효성 검사 (Data Validation):**
  - 파싱 후 데이터프레임의 컬럼 수가 예상(**116개**)과 일치하는지 확인합니다.
  - 핵심 측정값(위상각, 토크, 압입력 등) 컬럼이 숫자형(numeric) 데이터로 변환 가능한지 검증합니다. ('OK', 'NG' 등의 문자열 제외)
  - '일시' 컬럼이 유효한 날짜/시간 형식인지 확인하고, `datetime` 객체로 변환합니다.
  - 유효성 검사에 실패한 행이나 파일은 오류 로그를 기록하고, 처리를 중단하거나 별도 경로로 파일을 이동시킵니다.
- **데이터 정제 (Data Cleansing):**
  - 분석에 불필요한 '판정' 컬럼(`..._Result`) 등은 제거하거나 별도로 처리합니다.
  - 결측치(NaN)는 0으로 채우거나(`fillna(0)`), 데이터 특성에 따라 적절한 값으로 채웁니다.

## 4. 데이터 변환 및 저장 (Data Transformation & Storage)

- **데이터 변환 (Wide to Long):**
  - 파싱된 116개 컬럼의 와이드 포맷(wide format) 데이터를 분석 및 저장이 용이한 롱 포맷(long format)으로 변환합니다.
  - **예시:** `CAM1_Angle`, `CAM2_Angle`, ... `CAM9_Angle` 컬럼들을 `product_id`, `cam_number`, `angle` 세 개의 컬럼을 가진 여러 행으로 변환 (Unpivot/Melt).
  - 이 변환 과정을 통해 `database_schema.md`에 정의된 정규화된 테이블 구조에 맞게 데이터를 재구성합니다.
- **데이터 저장:**
  - **ORM:** `SQLAlchemy`를 사용하여 데이터베이스와의 상호작용을 처리함으로써 Oracle과 MariaDB 간의 호환성을 확보합니다.
  - **저장 방식:** 대량의 데이터를 효율적으로 삽입하기 위해 `bulk insert` 또는 `chunk-wise` 저장 방식을 사용합니다.
  - 트랜잭션을 적용하여 데이터 삽입 과정에서의 원자성을 보장합니다.

## 5. 오류 처리 및 로깅 (Error Handling & Logging)

- **오류 처리:** 파이프라인의 각 단계(수집, 파싱, 변환, 저장)에서 발생하는 모든 예외는 `try-except` 블록으로 처리합니다.
- **로깅:**
  - 오류 발생 시, 발생 시간, 파일 경로, 오류 메시지, 스택 트레이스를 포함한 상세 정보를 로그 파일에 기록합니다.
  - 정상 처리된 파일에 대해서도 처리 시간, 삽입된 행의 수 등을 정보 로그로 기록하여 파이프라인의 동작 상태를 모니터링할 수 있도록 합니다.
- **실패 처리:** 실패한 파일은 'failed' 폴더로 이동시키고, 재처리를 위한 방안을 마련합니다.
